æœ¬ç¯‡æ–‡ç« å°†ä»‹ç»å¦‚ä½•ä½¿ç”¨ `trl` åº“å¯¹ **Qwen-2.5** æ¨¡å‹è¿›è¡Œ **SFT LoRA** å¾®è°ƒã€‚
# 1. ç¯å¢ƒé…ç½®
å»ºè®®ä½¿ç”¨è™šæ‹Ÿç¯å¢ƒè¿›è¡Œé…ç½®ï¼ˆå¦‚ `conda`ï¼‰ï¼Œä»¥é¿å…ä¾èµ–å†²çªã€‚å¦‚æœä¸éœ€è¦è™šæ‹Ÿç¯å¢ƒï¼Œå¯ç›´æ¥è·³è‡³æ­¥éª¤ 2ã€‚
æ–°å»ºä¸€ä¸ª`conda`è™šæ‹Ÿç¯å¢ƒï¼š
```bash
conda create -n trl-train-env python=3.10
```

æ¿€æ´»è™šæ‹Ÿç¯å¢ƒï¼š
```bash
conda activate trl-train-env
```
# 2. ä¾èµ–åº“å®‰è£…
å®‰è£…`trl`åº“åŠç›¸å…³ä¾èµ–ï¼š
```bash
pip install trl peft
```

ç‰ˆæœ¬å‚è€ƒï¼š
```python
peft==0.16.0
transformers==4.52.4
trl==0.19.0
```
---
# 3. æ•°æ®å‡†å¤‡
ä»¥ **SFTTrainer** ä¸ºä¾‹ï¼Œéœ€å°†æ•°æ®é›†å¤„ç†ä¸ºå¦‚ä¸‹æ ¼å¼ï¼š
```python
[
	{
		'messages': [
			{'role': 'system', 'content': 'You are a helpful assistant.'},
			{'role': 'user', 'content': 'What is the capital of France?'},
			{'role': 'assistant', 'content': 'Paris is the capital of France.'}
		]
	},
	...
]
```
- æ¯æ¡æ•°æ®åŒ…å« `messages` å­—æ®µï¼Œå†…éƒ¨ä¸ºå¤šè½®å¯¹è¯ï¼ˆsystem/user/assistant è§’è‰²ï¼‰ã€‚
- è®­ç»ƒé›†ï¼ˆtrainï¼‰å’ŒéªŒè¯é›†ï¼ˆevalï¼‰æ ¼å¼ä¸€è‡´ã€‚


ç¤ºä¾‹æ•°æ®å¯åœ¨ [GitHub ä»“åº“](https://github.com/JohnWillian/trl_sfttrainer_tutorial.git) è·å–ï¼ˆæ¬¢è¿ **Star ğŸŒŸ**ï¼‰ï¼Œç¤ºä¾‹æ•°æ®æ ¼å¼å¦‚ä¸‹ï¼š
```python
[
	{
		'prompt': [{'role': 'user', 'content': '...'}],
		'completion': [{'role': 'assistant', 'content': '...'}]
	},
	...
]
```

éœ€è¦å°†å…¶å¤„ç†æˆ**SFTTrainer**è¦æ±‚çš„æ ¼å¼ï¼Œæ•°æ®å¤„ç†è„šæœ¬å¦‚ä¸‹ï¼š
```python
from datasets import load_dataset

# è¯·æ ¹æ®è‡ªå·±çš„æ•°æ®æ ¼å¼è‡ªå®šä¹‰ä¿®æ”¹
def formatting_prompts_func(example):
    example['messages'] = example['prompt'] + example['completion']
    return example

train_dataset_name = "./data/train"  # Note: è¿™é‡Œæ˜¯ä¸€ä¸ªç›®å½•ï¼Œä¸æ˜¯æ–‡ä»¶ã€‚ç›®å½•ä¸‹å­˜æ”¾è®­ç»ƒæ•°æ®ï¼Œä¾‹å¦‚`train.json`
val_dataset_name = "./data/eval"     # Note: è¿™é‡Œæ˜¯ä¸€ä¸ªç›®å½•ï¼Œä¸æ˜¯æ–‡ä»¶ã€‚ç›®å½•ä¸‹å­˜æ”¾éªŒè¯æ•°æ®ï¼Œä¾‹å¦‚`eval.json`

# è¯»å–trainæ•°æ®
train_dataset = load_dataset(train_dataset_name, split="train")
train_dataset = train_dataset.shuffle(seed=42)	# Shuffle æ•°æ®

# è¯»å–evalæ•°æ®
val_dataset = load_dataset(val_dataset_name, split="test")
val_dataset = val_dataset.shuffle(seed=42)	# Shuffle æ•°æ®

# Format train data
train_dataset = train_dataset.map(
    formatting_prompts_func,
    num_proc=4,
    remove_columns=['prompt', 'completion']
)

# Format eval data
val_dataset = val_dataset.map(
    formatting_prompts_func,
    num_proc=4,
    remove_columns=['prompt', 'completion']
```
> **SFTTrainer** å…¼å®¹ [**æ ‡å‡†**](https://huggingface.co/docs/trl/en/dataset_formats#standard)åŠ [**å¯¹è¯å¼**](https://huggingface.co/docs/trl/en/dataset_formats#conversational) æ•°æ®é›†æ ¼å¼ã€‚å½“è¾“å…¥å¯¹è¯æ•°æ®é›†æ—¶ï¼Œè®­ç»ƒå™¨ä¼šè‡ªåŠ¨å¯¹æ•°æ®é›†åº”ç”¨èŠå¤©æ¨¡æ¿ã€‚
---
# 4. Chat Templateè°ƒæ•´

åœ¨**SFTConfig**ä¸­ï¼Œè‹¥éœ€å¯ç”¨`assistant_only_loss`ï¼Œåˆ™ `chat_template` å¿…é¡»åŒ…å« `{% generation %}` æ ‡ç­¾ï¼ˆå‚è€ƒï¼š[Train on assistant messages only](https://huggingface.co/docs/trl/en/sft_trainer#train-on-assistant-messages-only)ï¼‰ã€‚åŸç‰ˆ **Qwen-2.5** æ¨¡å‹çš„æ¨¡æ¿æœªåŒ…å«æ­¤æ ‡ç­¾ï¼Œå› æ­¤éœ€è¦æ‰‹åŠ¨ä¿®æ”¹ï¼š
```python
from transformers import AutoTokenizer

base_model = "./models/Qwen/Qwen2.5-0.5B-Instruct"    # åŸºåº§æ¨¡å‹è·¯å¾„ï¼Œè‡ªè¡Œä¸‹è½½æ‰€éœ€æ¨¡å‹
modified_template = """
{%- if tools %}
    {{- '<|im_start|>system\\n' }}
    {%- if messages[0]['role'] == 'system' %}
        {{- messages[0]['content'] }}
    {%- else %}
        {{- 'You are Qwen, created by Alibaba Cloud. You are a helpful assistant.' }}
    {%- endif %}
    {{- "\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>" }}
    {%- for tool in tools %}
        {{- "\\n" }}
        {{- tool | tojson }}
    {%- endfor %}
    {{- "\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n" }}
{%- else %}
    {%- if messages[0]['role'] == 'system' %}
        {{- '<|im_start|>system\\n' + messages[0]['content'] + '<|im_end|>\\n' }}
    {%- else %}
        {{- '<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n' }}
    {%- endif %}
{%- endif %}
{%- for message in messages %}
    {%- if (message.role == "user") or (message.role == "system" and not loop.first) %}
        {{- '<|im_start|>' + message.role + '\\n' + message.content + '<|im_end|>' + '\\n' }}
    {%- elif (message.role == "assistant" and not message.tool_calls) %}
    {% generation %}    {{- '<|im_start|>' + message.role + '\\n' + message.content + '<|im_end|>' + '\\n' }}    {% endgeneration %}
    {%- elif message.role == "assistant" %}
        {% generation %}{{- '<|im_start|>' + message.role }}
        {%- if message.content %}
            {{- '\\n' + message.content }}
        {%- endif %}
        {%- for tool_call in message.tool_calls %}
            {%- if tool_call.function is defined %}
                {%- set tool_call = tool_call.function %}
            {%- endif %}
            {{- '\\n<tool_call>\\n{\\\"name\\\": \\\"' }}
            {{- tool_call.name }}
            {{- '\\\", \\\"arguments\\\": ' }}
            {{- tool_call.arguments | tojson }}
            {{- '}\\n</tool_call>' }}
        {%- endfor %}
        {{- '<|im_end|>\\n' }}{% endgeneration %}
    {%- elif message.role == "tool" %}
        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != "tool") %}
            {{- '<|im_start|>user' }}
        {%- endif %}
        {{- '\\n<tool_response>\\n' }}
        {{- message.content }}
        {{- '\\n</tool_response>' }}
        {%- if loop.last or (messages[loop.index0 + 1].role != "tool") %}
            {{- '<|im_end|>\\n' }}
        {%- endif %}
    {%- endif %}
{%- endfor %}
{%- if add_generation_prompt %}
    {% generation %}{{- '<|im_start|>assistant\\n' }}{% endgeneration %}
{%- endif %}
"""

# Load tokenizer
tokenizer = AutoTokenizer.from_pretrained(base_model)	# åŠ è½½Tokenizer
tokenizer.chat_template = modified_template		# ä¿®æ”¹ Chat Template
```
> å‚è€ƒï¼šhttps://github.com/huggingface/transformers/issues/34172

---
# 5. LoRA å‚æ•°é…ç½®
ä»¥ `rank=8`ï¼Œ`alpha=16` ä¸ºä¾‹ï¼š
```python
from peft import LoraConfig

# LoRA config
peft_config = LoraConfig(
    r=8,
    lora_alpha=16,
    lora_dropout=0.05,	# åº”ç”¨ dropoutï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
    bias="none",	# é»˜è®¤å€¼ï¼Œä¸ä½¿ç”¨ä»»ä½•åç½®
    target_modules="all-linear",	# ä½¿ç”¨ "all-linear" ä¼šå¯¹æ¨¡å‹ä¸­æ‰€æœ‰çº¿æ€§å±‚éƒ½æ·»åŠ  LoRA é€‚é…å™¨
    task_type="CAUSAL_LM"
)
```


- **rank**ï¼šä½ç§©çŸ©é˜µçš„ç»´åº¦ï¼Œå†³å®š LoRA é€‚é…å™¨çš„å®¹é‡ã€‚å€¼è¶Šå¤§ï¼Œæ¨¡å‹è¡¨è¾¾èƒ½åŠ›è¶Šå¼ºï¼Œä½†å‚æ•°å’Œæ˜¾å­˜å ç”¨ä¹Ÿæ›´é«˜ï¼Œå¯èƒ½è¿‡æ‹Ÿåˆï¼›å€¼è¶Šå°åˆ™æ›´è½»é‡ï¼Œä½†å¯èƒ½æ¬ æ‹Ÿåˆã€‚
- **alpha**ï¼šç¼©æ”¾å› å­ï¼Œæ§åˆ¶ LoRA æ›´æ–°å¯¹åŸå§‹æƒé‡çš„å½±å“å¼ºåº¦ã€‚å®é™…æ›´æ–°é‡ä¸º $(\alpha / r) \times \Delta W$ï¼Œå› æ­¤ $\alpha / r$ æ‰æ˜¯å®é™…çš„å­¦ä¹ ç‡æ¯”ä¾‹ã€‚

---
# 6. è®­ç»ƒå‚æ•°é…ç½®

```python
from trl import SFTConfig

# Training Config
training_arguments = SFTConfig(
    assistant_only_loss=True,   # åªè®¡ç®—assistantçš„loss
    output_dir='./output/finetune/qwen25_14b_sft_lora_r8_a16',   # è‡ªå®šä¹‰æ¨¡å‹ä¿å­˜è·¯å¾„
    per_device_train_batch_size=1,  # éœ€è¦ä¸deepspeedé…ç½®æ–‡ä»¶ä¿æŒä¸€è‡´
    per_device_eval_batch_size=1,   # éœ€è¦ä¸Training Configä¿æŒä¸€è‡´
    gradient_accumulation_steps=8,  # éœ€è¦ä¸deepspeedé…ç½®æ–‡ä»¶ä¿æŒä¸€è‡´
    optim="paged_adamw_8bit",   # ä¼˜åŒ–å™¨ï¼Œä½¿ç”¨paged_adamwè®­ç»ƒé€Ÿåº¦æ›´å¿«
    num_train_epochs=2,   # è®­ç»ƒè½®æ•°
    gradient_checkpointing=True,    # æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼Œé™ä½æ˜¾å­˜æ¶ˆè€—
    do_eval=True,   # æ˜¯å¦è¿›è¡Œè¯„ä¼°
    eval_strategy="steps",
    eval_steps=50,  # æ¯50æ­¥è¯„ä¼°ä¸€æ¬¡
    save_strategy="steps",  # ä¿å­˜ç­–ç•¥ä¸ºsteps
    save_steps=50,  # æ¯50æ­¥ä¿å­˜ä¸€æ¬¡
    logging_steps=1,    # logging
    logging_strategy="steps",
    warmup_ratio=0.03,
    learning_rate=2e-5,
    bf16=True,  # éœ€è¦ä¸deepspeedé…ç½®æ–‡ä»¶ä¿æŒä¸€è‡´
    lr_scheduler_type="cosine", # Default is `linear`
    max_length=4096,    # æ ·æœ¬åºåˆ—æœ€å¤§é•¿åº¦
    label_names=["labels"],
    report_to = "tensorboard",  # show tensorboard
)
```

---

# 7. å¼€å§‹è®­ç»ƒ

```python
from trl import SFTTrainer

trainer = SFTTrainer(
    model=base_model,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    args=training_arguments,
    processing_class=tokenizer, # tokenizer
    peft_config=peft_config,    # lora config
)

trainer.train()
```

---

# 8. DeepSpeed é…ç½®
è‹¥æ˜¾å­˜æœ‰é™ï¼Œæ¨èä½¿ç”¨ DeepSpeed è¿›è¡Œå¹¶è¡Œè®­ç»ƒã€‚ä»¥ `zero_1` é…ç½®ä¸ºä¾‹ï¼š
```yaml
compute_environment: LOCAL_MACHINE
debug: false
deepspeed_config:
  deepspeed_multinode_launcher: standard
  per_device_train_batch_size: 1  # éœ€è¦ä¸Training Configä¿æŒä¸€è‡´
  gradient_accumulation_steps: 8  # éœ€è¦ä¸Training Configä¿æŒä¸€è‡´
  zero3_init_flag: false
  zero_stage: 1
distributed_type: DEEPSPEED
downcast_bf16: 'no'
machine_rank: 0
main_training_function: main
mixed_precision: 'bf16' # éœ€è¦ä¸Training Configä¿æŒä¸€è‡´
num_machines: 1 # 1 machine
num_processes: 8  # ä½¿ç”¨ 8 GPUsï¼Œæ ¹æ®å®é™…æƒ…å†µè‡ªè¡Œä¿®æ”¹
rdzv_backend: static
same_network: true
tpu_env: []
tpu_use_cluster: false
tpu_use_sudo: false
use_cpu: false
```
å°†é…ç½®æ–‡ä»¶ä¿å­˜ä¸º `ds_zero1_config.yaml`ï¼Œç„¶åï¼Œå°†å‰é¢è®²åˆ°çš„è®­ç»ƒé…ç½®ä»£ç æ•´ç†ä¸ºä¸€ä¸ªæ–‡ä»¶ï¼š
```python
from transformers import AutoTokenizer
from peft import LoraConfig
from datasets import load_dataset
from trl import SFTTrainer, SFTConfig

base_model = "./models/Qwen/Qwen2.5-0.5B-Instruct"    # åŸºåº§æ¨¡å‹è·¯å¾„ï¼Œè‡ªè¡Œä¸‹è½½æ‰€éœ€æ¨¡å‹
train_dataset_name = "./data/train" # Note: è¿™é‡Œæ˜¯ä¸€ä¸ªç›®å½•ï¼Œä¸æ˜¯æ–‡ä»¶ã€‚ç›®å½•ä¸‹å­˜æ”¾è®­ç»ƒæ•°æ®ï¼Œä¾‹å¦‚`train.json`
val_dataset_name = "./data/eval"    # Note: è¿™é‡Œæ˜¯ä¸€ä¸ªç›®å½•ï¼Œä¸æ˜¯æ–‡ä»¶ã€‚ç›®å½•ä¸‹å­˜æ”¾éªŒè¯æ•°æ®ï¼Œä¾‹å¦‚`eval.json`
new_model = "../output/finetune/qwen25_05b_sft_lora_r8_a16"   # è®­ç»ƒå¥½çš„æ¨¡å‹ä¿å­˜è·¯å¾„

modified_template = """
{%- if tools %}
    {{- '<|im_start|>system\\n' }}
    {%- if messages[0]['role'] == 'system' %}
        {{- messages[0]['content'] }}
    {%- else %}
        {{- 'You are Qwen, created by Alibaba Cloud. You are a helpful assistant.' }}
    {%- endif %}
    {{- "\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>" }}
    {%- for tool in tools %}
        {{- "\\n" }}
        {{- tool | tojson }}
    {%- endfor %}
    {{- "\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n" }}
{%- else %}
    {%- if messages[0]['role'] == 'system' %}
        {{- '<|im_start|>system\\n' + messages[0]['content'] + '<|im_end|>\\n' }}
    {%- else %}
        {{- '<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n' }}
    {%- endif %}
{%- endif %}
{%- for message in messages %}
    {%- if (message.role == "user") or (message.role == "system" and not loop.first) %}
        {{- '<|im_start|>' + message.role + '\\n' + message.content + '<|im_end|>' + '\\n' }}
    {%- elif (message.role == "assistant" and not message.tool_calls) %}
    {% generation %}    {{- '<|im_start|>' + message.role + '\\n' + message.content + '<|im_end|>' + '\\n' }}    {% endgeneration %}
    {%- elif message.role == "assistant" %}
        {% generation %}{{- '<|im_start|>' + message.role }}
        {%- if message.content %}
            {{- '\\n' + message.content }}
        {%- endif %}
        {%- for tool_call in message.tool_calls %}
            {%- if tool_call.function is defined %}
                {%- set tool_call = tool_call.function %}
            {%- endif %}
            {{- '\\n<tool_call>\\n{\\\"name\\\": \\\"' }}
            {{- tool_call.name }}
            {{- '\\\", \\\"arguments\\\": ' }}
            {{- tool_call.arguments | tojson }}
            {{- '}\\n</tool_call>' }}
        {%- endfor %}
        {{- '<|im_end|>\\n' }}{% endgeneration %}
    {%- elif message.role == "tool" %}
        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != "tool") %}
            {{- '<|im_start|>user' }}
        {%- endif %}
        {{- '\\n<tool_response>\\n' }}
        {{- message.content }}
        {{- '\\n</tool_response>' }}
        {%- if loop.last or (messages[loop.index0 + 1].role != "tool") %}
            {{- '<|im_end|>\\n' }}
        {%- endif %}
    {%- endif %}
{%- endfor %}
{%- if add_generation_prompt %}
    {% generation %}{{- '<|im_start|>assistant\\n' }}{% endgeneration %}
{%- endif %}
"""

# Load tokenizer
tokenizer = AutoTokenizer.from_pretrained(base_model)
tokenizer.chat_template = modified_template
tokenizer.pad_token = tokenizer.eos_token
tokenizer.pad_token_id = tokenizer.eos_token_id

# LoRA config
peft_config = LoraConfig(
    r=8,
    lora_alpha=16,
    lora_dropout=0.05,
    bias="none",
    target_modules="all-linear",
    task_type="CAUSAL_LM"
)

# ç»„åˆæˆ conversational format
# conversational format æ ·ä¾‹ï¼š
# 'messages': [
#     {'role': 'system', 'content': 'You are a helpful assistant.'},
#     {'role': 'user', 'content': 'What is the capital of France?'},
#     {'role': 'assistant', 'content': 'Paris is the capital of France.'}
# ]
# è¯·æ ¹æ®è‡ªå·±çš„æ•°æ®æ ¼å¼è‡ªå®šä¹‰ä¿®æ”¹
def formatting_prompts_func(example):
    example['messages'] = example['prompt'] + example['completion']
    return example

def main():
    # è¯»å–trainæ•°æ®
    train_dataset = load_dataset(train_dataset_name, split="train")
    train_dataset = train_dataset.shuffle(seed=42)	# Shuffle æ•°æ®

    # è¯»å–evalæ•°æ®
    val_dataset = load_dataset(val_dataset_name, split="test")
    val_dataset = val_dataset.shuffle(seed=42)	# Shuffle æ•°æ®

    # Format train data
    train_dataset = train_dataset.map(
        formatting_prompts_func,
        num_proc=4,
        remove_columns=['prompt', 'completion']
    )

    # Format eval data
    val_dataset = val_dataset.map(
        formatting_prompts_func,
        num_proc=4,
        remove_columns=['prompt', 'completion']
    )

    # Training Config
    training_arguments = SFTConfig(
        assistant_only_loss=True,   # åªè®¡ç®—assistantçš„loss
        output_dir=new_model,   # æ¨¡å‹ä¿å­˜è·¯å¾„
        per_device_train_batch_size=1,  # éœ€è¦ä¸deepspeedé…ç½®æ–‡ä»¶ä¿æŒä¸€è‡´
        per_device_eval_batch_size=1,   # éœ€è¦ä¸Training Configä¿æŒä¸€è‡´
        gradient_accumulation_steps=8,  # éœ€è¦ä¸deepspeedé…ç½®æ–‡ä»¶ä¿æŒä¸€è‡´
        optim="paged_adamw_8bit",   # ä¼˜åŒ–å™¨ï¼Œä½¿ç”¨paged_adamwè®­ç»ƒé€Ÿåº¦æ›´å¿«
        num_train_epochs=3,   # è®­ç»ƒè½®æ•°
        gradient_checkpointing=True,    # æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼Œé™ä½æ˜¾å­˜æ¶ˆè€—
        do_eval=True,   # æ˜¯å¦è¿›è¡Œè¯„ä¼°
        eval_strategy="steps",
        eval_steps=50,  # æ¯50æ­¥è¯„ä¼°ä¸€æ¬¡
        save_strategy="steps",  # ä¿å­˜ç­–ç•¥ä¸ºsteps
        save_steps=50,  # æ¯50æ­¥ä¿å­˜ä¸€æ¬¡
        logging_steps=1,    # logging
        logging_strategy="steps",
        warmup_ratio=0.03,
        learning_rate=2e-5,
        bf16=True,  # éœ€è¦ä¸deepspeedé…ç½®æ–‡ä»¶ä¿æŒä¸€è‡´
        lr_scheduler_type="cosine", # Default is `linear`
        max_length=4096,    # æ ·æœ¬åºåˆ—æœ€å¤§é•¿åº¦
        label_names=["labels"],
        report_to = "tensorboard",  # show tensorboard
    )

    trainer = SFTTrainer(
        model=base_model,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
        args=training_arguments,
        processing_class=tokenizer, # tokenizer
        peft_config=peft_config,    # lora config
    )

    trainer.train()

if __name__ == "__main__":
    main()
```

ä¿å­˜ä¸º`train_sft_lora_qwen2.py`æ–‡ä»¶ï¼Œå¹¶ç»“åˆ`accelerate`å¯åŠ¨å¹¶è¡Œè®­ç»ƒï¼š
```bash
CUDA_VISIBLE_DEVICES="0,1,2,3,4,5,6,7" accelerate launch --config_file ds_zero1_config.yaml train_sft_lora_qwen2.py
```
> `CUDA_VISIBLE_DEVICES` ç”¨äºæŒ‡å®š CUDA åº”ç”¨ç¨‹åºå¯è§çš„ GPU è®¾å¤‡ã€‚å¦‚æœéœ€è¦è°ƒæ•´è¯¥å‚æ•°ï¼Œè¯·åŒæ—¶ä¿®æ”¹ deepspeed é…ç½®æ–‡ä»¶ä¸­çš„ `num_processes` å‚æ•°ï¼Œä½¿ä¸¤è€…ä¿æŒä¸€è‡´ã€‚

æ›´å¤šé…ç½®å’Œä½¿ç”¨æ–¹å¼è¯·å‚è€ƒ [GitHub ä»“åº“](https://github.com/JohnWillian/trl_sfttrainer_tutorial.git)ã€‚

---

å¦‚æœè§‰å¾—è¿™ç¯‡æ–‡ç« æœ‰ç”¨ï¼Œå°±ç»™ä¸ª**èµ**ğŸ‘å’Œ**æ”¶è—**â­ï¸å§ï¼ä¹Ÿæ¬¢è¿åœ¨è¯„è®ºåŒºåˆ†äº«ä½ çš„çœ‹æ³•ï¼

---

# 9. å‚è€ƒ
- http://huggingface.co/docs/trl/en/sft_trainer
- https://huggingface.co/docs/trl/en/sft_trainer#train-on-assistant-messages-only
- https://github.com/huggingface/transformers/issues/34172